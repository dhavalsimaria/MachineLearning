{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Xx7CCJZKdcC2"
   },
   "source": [
    "# Building a Chatbot from Scratch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JsR2PuNPdcC5"
   },
   "source": [
    "##### In this project we will build a chatbot from scratch using the corenell University's Movie Dialogue corpus.\n",
    "##### We will be using a deep learning based architecture with the main components as a lstm based encoder and decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "V7b5o0oxdcC7",
    "outputId": "60661f58-4aa8-45e0-bdf7-a8115881b505"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers.recurrent import LSTM\n",
    "from keras.layers import Dense, Input, Embedding\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from collections import Counter\n",
    "import nltk\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_WFN5fXsdcDB"
   },
   "outputs": [],
   "source": [
    "import keras\n",
    "import nltk\n",
    "import numpy\n",
    "import sklearn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1IQF3dSidcDI"
   },
   "source": [
    "Please make sure that the version of the respective packages are met to the requirement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oCL0434GdcDK"
   },
   "outputs": [],
   "source": [
    "assert keras.__version__=='2.1.2'\n",
    "assert nltk.__version__=='3.4.1'\n",
    "assert sklearn.__version__=='0.21.2'\n",
    "assert numpy.__version__=='1.12.1'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gv38ReVtdcDO"
   },
   "source": [
    "Download the glove model available at https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "Specification : Twitter (2B tweets, 27B tokens, 1.2M vocab, uncased, 25d, 50d, 100d, & 200d vectors, 1.42 GB download): glove.twitter.27B.zip\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KXuwJNpGdcDQ"
   },
   "source": [
    "you can download it with 'wget' or can directly put the embedding zip file inside 'embedding_data' folder and unzip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VHx1FnytdcDU",
    "outputId": "f76dd077-9502-4dd3-cbc2-836180e2bfb8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0curl: (6) Could not resolve host: downloads.cs.stanford.edu\n"
     ]
    }
   ],
   "source": [
    "! curl -O http://downloads.cs.stanford.edu/nlp/data/glove.twitter.27B.zip "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F_yG4mV2dcDZ"
   },
   "outputs": [],
   "source": [
    "RAND_STATE=np.random.seed(42)\n",
    "BATCH_SIZE = 32\n",
    "NUM_EPOCHS = 10\n",
    "GLOVE_EMBEDDING_SIZE = 100\n",
    "HIDDEN_UNITS = 256\n",
    "MAX_INPUT_SEQ_LENGTH = 40\n",
    "MAX_TARGET_SEQ_LENGTH = 40\n",
    "MAX_VOCAB_SIZE = 10000\n",
    "DATA_SET_NAME = 'cornell'\n",
    "DATA_PATH = './cornell/movie_lines_cleaned.txt'\n",
    "GLOVE_MODEL = \"./embedding_data/glove.twitter.27B.100d.txt\"\n",
    "WHITELIST = 'abcdefghijklmnopqrstuvwxyz1234567890?.,'\n",
    "WEIGHT_FILE_PATH =  DATA_SET_NAME + '/word-glove-weights.h5'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nsku9GyddcDe"
   },
   "outputs": [],
   "source": [
    "def in_white_list(_word):\n",
    "  '''Check if the characters in the words are whitelisted'''\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xKA3WnoNdcDk"
   },
   "source": [
    "Load the glove word embedding in to a dictionary where the **key** is a unique **word token** and the **value** is a **d** dimension vector "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LQ6Q6VpHdcDn"
   },
   "source": [
    "# Test-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_DtZYF8_dcDp"
   },
   "outputs": [],
   "source": [
    "def load_glove_vector():\n",
    "    _word2embedding = {}\n",
    "    file = open(GLOVE_MODEL, mode='rt', encoding='utf8')\n",
    "    for line in file:\n",
    "        '''write here. write your code to load the data in to the dictionary\n",
    "        make sure the value is a numpy array of size 100\n",
    "        max  3 to 6 lines of code'''\n",
    "    file.close()\n",
    "    return _word2embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "jdoRZsWEdcDs"
   },
   "outputs": [],
   "source": [
    "word2embedding = load_glove_vector()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Wu1WwP20dcDw"
   },
   "source": [
    "# Check-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VknKfgmGdcDz"
   },
   "outputs": [],
   "source": [
    "assert len(word2embedding.keys())==1193513\n",
    "for key in word2embedding.keys():\n",
    "    try:\n",
    "        assert len(word2embedding[key])==100\n",
    "    except AssertionError:\n",
    "        print (key,len(word2embedding[key]))     "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mCix18rAdcD7"
   },
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bkKBqppEdcD8"
   },
   "outputs": [],
   "source": [
    "target_counter = Counter()\n",
    "lines = open(DATA_PATH, 'rt', encoding='utf8').read().split('\\n')\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "prev_words = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "0FsdjO_jdcD_"
   },
   "outputs": [],
   "source": [
    "for line in lines:\n",
    "    next_words = [w.lower() for w in nltk.word_tokenize(line)]\n",
    "    if len(next_words) > MAX_TARGET_SEQ_LENGTH:\n",
    "        next_words = next_words[0:MAX_TARGET_SEQ_LENGTH]\n",
    "    if len(prev_words) > 0:\n",
    "        input_texts.append(prev_words)\n",
    "        target_words = next_words[:]\n",
    "        target_words.insert(0, 'start')\n",
    "        target_words.append('end')\n",
    "        for w in target_words:\n",
    "            target_counter[w] += 1\n",
    "        target_texts.append(target_words)\n",
    "    prev_words = next_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LcYFBgUIdcEF"
   },
   "source": [
    "Filter the conversations till max word length and convert the dialogues pairs into input text and target texts. Put **start** and **end** token to recognise the beginning and end of the sentence token.\n",
    "\n",
    "## Let's see some of the training examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "FiCxfQtBdcEG",
    "outputId": "220d92c7-6aa1-4962-9c2c-c853de960b9a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['they', 'do', 'not', '!'], ['start', 'they', 'do', 'to', '!', 'end']]\n",
      "[['they', 'do', 'to', '!'], ['start', 'i', 'hope', 'so', '.', 'end']]\n",
      "[['i', 'hope', 'so', '.'], ['start', 'she', 'okay', '?', 'end']]\n",
      "[['she', 'okay', '?'], ['start', 'let', \"'s\", 'go', '.', 'end']]\n",
      "[['let', \"'s\", 'go', '.'], ['start', 'wow', 'end']]\n",
      "[['wow'], ['start', 'okay', '--', 'you', \"'re\", 'gon', 'na', 'need', 'to', 'learn', 'how', 'to', 'lie', '.', 'end']]\n",
      "[['okay', '--', 'you', \"'re\", 'gon', 'na', 'need', 'to', 'learn', 'how', 'to', 'lie', '.'], ['start', 'no', 'end']]\n",
      "[['no'], ['start', 'i', \"'m\", 'kidding', '.', 'you', 'know', 'how', 'sometimes', 'you', 'just', 'become', 'this', '``', 'persona', \"''\", '?', 'and', 'you', 'do', \"n't\", 'know', 'how', 'to', 'quit', '?', 'end']]\n",
      "[['i', \"'m\", 'kidding', '.', 'you', 'know', 'how', 'sometimes', 'you', 'just', 'become', 'this', '``', 'persona', \"''\", '?', 'and', 'you', 'do', \"n't\", 'know', 'how', 'to', 'quit', '?'], ['start', 'like', 'my', 'fear', 'of', 'wearing', 'pastels', '?', 'end']]\n",
      "[['like', 'my', 'fear', 'of', 'wearing', 'pastels', '?'], ['start', 'the', '``', 'real', 'you', \"''\", '.', 'end']]\n",
      "[['the', '``', 'real', 'you', \"''\", '.'], ['start', 'what', 'good', 'stuff', '?', 'end']]\n"
     ]
    }
   ],
   "source": [
    "for idx, (input_words, target_words) in enumerate(zip(input_texts, target_texts)):\n",
    "    if idx > 10:\n",
    "        break\n",
    "    print([input_words, target_words])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ksKxizS6dcEL"
   },
   "source": [
    "### Create two dictionaries \n",
    "<ol>\n",
    "<li>target_word2id\n",
    "<li>target_id2word\n",
    "</ol>\n",
    "and save it as NumPy file format in the disk.\n",
    "<p>\n",
    "<strong>NOTE:</strong> The ids should start from 1 beacause <strong>0</strong> is reserved for <strong>'unknown'</strong> tokens.\n",
    "Make sure you cosider only the <strong>most common</strong> tokens with <strong>MAX_VOCAB_SIZE</strong> defined above.\n",
    "\n",
    "Most common refers to tokens with higher frequency. \n",
    "</p>\n",
    "<strong>Help:</strong>\n",
    "<ol>\n",
    "<li>Use the target_counter which have the token counts.  \n",
    "<li>Use target_counter.most_common(MAX_VOCAB_SIZE) to filter common tokens\n",
    "    </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Qc6oco8qdcEN"
   },
   "outputs": [],
   "source": [
    "target_word2idx = dict()\n",
    "'''create a target word to id dictionary called target_word2idx.\n",
    "2 to 3 lines '''\n",
    "\n",
    "if 'unk' not in target_word2idx:\n",
    "    target_word2idx['unk'] = 0\n",
    "\n",
    "'''create a target to id dictionary called target_idx2word . Approx ~1 line'''\n",
    "\n",
    "\n",
    "\n",
    "np.save( DATA_SET_NAME + '/word-glove-target-word2idx.npy', target_word2idx)\n",
    "np.save( DATA_SET_NAME + '/word-glove-target-idx2word.npy', target_idx2word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "6PzuUg4zdcER"
   },
   "source": [
    "# Check-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R7pikaJfdcEU"
   },
   "outputs": [],
   "source": [
    "assert len (target_word2idx.keys())==len (target_idx2word.keys())==MAX_VOCAB_SIZE+1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2mw-Ly2xdcEc"
   },
   "source": [
    "# Prepare the input data with embedding\n",
    "The input data is a list of lists \n",
    "<ol>\n",
    "<li> First list is a list of sentences\n",
    "<li> Each sentence is a list of words\n",
    " </ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oBmG1ea4dcEc",
    "outputId": "55abd364-d514-47cf-cadf-fc456085bab8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'num_decoder_tokens': 10002, 'encoder_max_seq_length': 0, 'decoder_max_seq_length': 42}\n"
     ]
    }
   ],
   "source": [
    "input_texts_word2em = []\n",
    "encoder_max_seq_length = 0\n",
    "decoder_max_seq_length = 0\n",
    "\n",
    "for input_words, target_words in zip(input_texts, target_texts):\n",
    "    encoder_input_wids = []\n",
    "    for w in input_words:\n",
    "        '''enter your code here.\n",
    "        '''\n",
    "\n",
    "    input_texts_word2em.append(encoder_input_wids)\n",
    "    encoder_max_seq_length = max(len(encoder_input_wids), encoder_max_seq_length)\n",
    "    decoder_max_seq_length = max(len(target_words), decoder_max_seq_length)\n",
    "\n",
    "context = dict()\n",
    "context['num_decoder_tokens'] = num_decoder_tokens\n",
    "context['encoder_max_seq_length'] = encoder_max_seq_length\n",
    "context['decoder_max_seq_length'] = decoder_max_seq_length\n",
    "\n",
    "print(context)\n",
    "np.save( DATA_SET_NAME + '/word-glove-context.npy', context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "S8IJeZVkdcEg"
   },
   "source": [
    "# Check-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QXaPOQlddcEh"
   },
   "outputs": [],
   "source": [
    "for input_text,input_text_embed in zip (input_texts,range(len(input_texts_word2em))):\n",
    "    assert (len(input_text)==len(input_texts_word2em[input_text_embed]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GXh-5WEUdcEn"
   },
   "source": [
    "# Generate Training data per batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Gvg8LBt-dcEo"
   },
   "source": [
    "generate_batch takes input embedding data (input_word2em_data) and target text data (target_texts) and returns trainable X and Y.\n",
    "X is a list of [X1,X2]\n",
    "where \n",
    "X1 is encoder_input_data_batch( which is created by putting the word embedding(glove vector) of the input tokens) padded in to a shape of (BATCH_SIZE, encoder_max_seq_length, GLOVE_EMBEDDING_SIZE)\n",
    "\n",
    "X2 is decoder_input_data_batch which is created by putting the word embedding(glove vector) of the target_words tokens and padding it to a shape of (BATCH_SIZE, encoder_max_seq_length, GLOVE_EMBEDDING_SIZE)\n",
    "\n",
    "Y is decoder_target_data_batch which is in shape of (BATCH_SIZE, decoder_max_seq_length, num_decoder_tokens)\n",
    "which signifies for each target token text  in the batch we have an option of any token from the vocabularu to be the next predicted word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "37R_9qpHdcEp"
   },
   "outputs": [],
   "source": [
    "def generate_batch(input_word2em_data, output_text_data):\n",
    "    num_batches = len(input_word2em_data) // BATCH_SIZE\n",
    "    while True:\n",
    "        for batchIdx in range(0, num_batches):\n",
    "            start = batchIdx * BATCH_SIZE\n",
    "            end = (batchIdx + 1) * BATCH_SIZE\n",
    "            '''Fill your code here. 5 to 10 lines'''\n",
    "            yield [encoder_input_data_batch, decoder_input_data_batch], decoder_target_data_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EGjnjiyidcEt"
   },
   "source": [
    "# Check-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Ezag0feRdcEu"
   },
   "outputs": [],
   "source": [
    "Xtrain, Xtest, Ytrain, Ytest = train_test_split(input_texts_word2em, target_texts, test_size=0.2, random_state=42)\n",
    "train_gen = generate_batch(Xtrain, Ytrain)\n",
    "for i,j in train_gen:\n",
    "    assert i[0].shape==(BATCH_SIZE,context['encoder_max_seq_length'],GLOVE_EMBEDDING_SIZE)\n",
    "    assert i[1].shape==(BATCH_SIZE,context['decoder_max_seq_length'],GLOVE_EMBEDDING_SIZE)\n",
    "    assert j.shape==    (BATCH_SIZE,context['decoder_max_seq_length'],context['num_decoder_tokens'])\n",
    "\n",
    "print ('Test Case 4 Passes!')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jazC-goddcEx"
   },
   "source": [
    "# Model Architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "lab5ddRGdcEy",
    "outputId": "d714ab20-50ac-459b-f71a-899be270657d"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-c90fac7e13e3>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mplot_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mto_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'model.png'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mIPython\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdisplay\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mImage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'model.png'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m400\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')\n",
    "from IPython.display import Image\n",
    "Image(filename='model.png',height=400,width=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FKnqwC4ndcE4"
   },
   "source": [
    "# The Model architecture is explined in the diagram above "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P8VfVTm4dcE6"
   },
   "source": [
    "# Test-5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oUJbRP9SdcE8"
   },
   "source": [
    "<ol>\n",
    "<li> Step 1: Use a LSTM encoder to get input words encoded in the form of (encoder outputs, encoder hidden state, encoder context) from input words\n",
    "<li> Step 2:  Use a LSTM decoder to get target words encoded in the form of (decoder outputs, decoder hidden state, decoder context) from target words. Use encoder hidden states and encoder context (represents input memory) as initial state .\n",
    "<li> Step 3: Use a dense layer to predict the next token out of the vocabulary given decoder output generated by Step 2.\n",
    "<li> Step 4: Use loss ='categorical_crossentropy' and optimizer='rmsprop'\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gU974rtIdcE9"
   },
   "outputs": [],
   "source": [
    "'''write your code here.\n",
    "   create a model object'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pnNAPfQFdcFG"
   },
   "source": [
    "# Check-5 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ecKi6g5MdcFI"
   },
   "source": [
    "Check the model summary should look like this "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "x2-ZeC66dcFJ",
    "outputId": "c2c29a26-2f9a-4d1e-96ed-a6408dbb1456"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "encoder_inputs (InputLayer)     (None, None, 100)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "decoder_inputs (InputLayer)     (None, None, 100)    0                                            \n",
      "__________________________________________________________________________________________________\n",
      "encoder_lstm (LSTM)             [(None, 256), (None, 365568      encoder_inputs[0][0]             \n",
      "__________________________________________________________________________________________________\n",
      "decoder_lstm (LSTM)             [(None, None, 256),  365568      decoder_inputs[0][0]             \n",
      "                                                                 encoder_lstm[0][1]               \n",
      "                                                                 encoder_lstm[0][2]               \n",
      "__________________________________________________________________________________________________\n",
      "decoder_dense (Dense)           (None, None, 10002)  2570514     decoder_lstm[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 3,301,650\n",
      "Trainable params: 3,301,650\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eOTTSdcndcFS"
   },
   "source": [
    "# Prediction"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Copy of Chatbot.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
