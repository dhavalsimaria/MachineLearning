{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import gym\n",
    "# Importing libraries\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "from collections import deque\n",
    "import collections\n",
    "import pickle\n",
    "\n",
    "#for text processing\n",
    "import spacy\n",
    "import re\n",
    "import pandas as pd\n",
    "env = gym.make(\"Taxi-v3\").env\n",
    "\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### There are 4 locations (labeled by different letters), and our job is to pick up the passenger at one location and drop him off at another. We receive +20 points for a successful drop-off and lose 1 point for every time-step it takes. There is also a 10 point penalty for illegal pick-up and drop-off actions.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetching Origing, Destination, and Time of Pickup from the sms data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_pickup_drop(text):\n",
    "    origin = ''\n",
    "    destination = ''\n",
    "    time_of_pickup = ''\n",
    "    parts_of_sentence = re.split(r' to ', text)\n",
    "    for part in parts_of_sentence:\n",
    "        for loc in loc_dict:\n",
    "            if((loc in part) and ('from' in part)):\n",
    "               origin = loc\n",
    "            elif((loc in part) and ('from' not in part)):\n",
    "                destination = loc\n",
    "    \n",
    "    return [origin, destination, time_of_pickup]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+\n",
      "|R: | : :G|\n",
      "| : | : : |\n",
      "| : : : : |\n",
      "| | : | : |\n",
      "|Y| : |B: |\n",
      "+---------+\n",
      "\n",
      "Action Space Discrete(6)\n",
      "State Space Discrete(500)\n"
     ]
    }
   ],
   "source": [
    "env.reset() # reset environment to a new, random state\n",
    "env.render()\n",
    "\n",
    "print(\"Action Space {}\".format(env.action_space))\n",
    "print(\"State Space {}\".format(env.observation_space))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summing up the Q-Learning Process\n",
    "Breaking it down into steps, we get\n",
    "\n",
    "Initialize the Q-table by all zeros.\n",
    "\n",
    "Start exploring actions: \n",
    "\n",
    "For each state, select any one among all possible actions for the current state (S).\n",
    "\n",
    "Travel to the next state (S') as a result of that action (a).\n",
    "\n",
    "For all possible actions from the state (S') select the one with the highest Q-value.\n",
    "\n",
    "Update Q-table values using the equation.\n",
    "\n",
    "Set the next state as the current state.\n",
    "\n",
    "If goal state is reached, then end and repeat the process.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploiting learned values\n",
    "After enough random exploration of actions, the Q-values tend to converge serving our agent as an action-value function which it can exploit to pick the most optimal action from a given state.\n",
    "\n",
    "There's a tradeoff between exploration (choosing a random action) and exploitation (choosing actions based on already learned Q-values). We want to prevent the action from always taking the same route, and possibly overfitting, so we'll be introducing another parameter called Ïµ \"epsilon\" to cater to this during training.\n",
    "\n",
    "Instead of just selecting the best learned Q-value action, we'll sometimes favor exploring the action space further. Lower epsilon value results in episodes with more penalties (on average) which is obvious because we are exploring and making random decisions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initialize Q_table\n",
    "import numpy as np\n",
    "\n",
    "q_table = np.zeros([env.observation_space.n, env.action_space.n])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 100000\n",
      "Wall time: 1min 32s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\"\"\"Training the agent\"\"\"\n",
    "\n",
    "import random\n",
    "from IPython.display import clear_output\n",
    "\n",
    "# Hyperparameters\n",
    "alpha = 0.1\n",
    "gamma = 0.6\n",
    "epsilon = 0.1\n",
    "\n",
    "# For plotting metrics\n",
    "all_epochs = []\n",
    "all_penalties = []\n",
    "\n",
    "##Write your code here\n",
    "for i in range(1, 100001):\n",
    "    state = env.reset()\n",
    "    \n",
    "    epochs, penalties, reward, = 0, 0, 0\n",
    "    done = False\n",
    "    \n",
    "    while not done:\n",
    "        if random.uniform(0,1) < epsilon:\n",
    "            action = env.action_space.sample()   # Explore action space\n",
    "        else:\n",
    "            action = np.argmax(q_table[state])   # Exploit learned values\n",
    "            \n",
    "        next_state, reward, done, info = env.step(action)\n",
    "        \n",
    "        old_value = q_table[state, action]\n",
    "        next_max = np.max(q_table[next_state])\n",
    "        \n",
    "        new_value = (1 - alpha) * old_value + alpha * (reward + gamma * next_max)\n",
    "        q_table[state, action] = new_value\n",
    "        \n",
    "        if reward == -10:\n",
    "            penalties += 1\n",
    "        \n",
    "        state = next_state\n",
    "        epochs += 1\n",
    "        \n",
    "    if i % 100 == 0:\n",
    "        clear_output(wait=True)\n",
    "        print(f\"Episode: {i}\")\n",
    "        \n",
    "\n",
    "np.save(\"./q_table.npy\", q_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load trained q_table for evaluation\n",
    "\n",
    "q_table = np.load(\"./q_table.npy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_loc_dict(city_df):\n",
    "    loc_dict = {}\n",
    "    ## Create dictionary example, loc_dict['dwarka sector 23] = 0\n",
    "    for i in range(len(city_df)):\n",
    "        loc_dict[city_df.loc[i, 'location']] = city_df.loc[i, 'mapping']\n",
    "        \n",
    "    return loc_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_pick_up_drop_correction(pick_up, drop, line_num):\n",
    "    orig_df = pd.read_csv(\"./org_df.csv\")\n",
    "    original_origin = orig_df.loc[line_num, 'origin']\n",
    "    original_destination = orig_df.loc[line_num, 'dest']\n",
    "\n",
    "    if original_origin == pick_up and original_destination == drop:\n",
    "        return True\n",
    "    else:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: './sms.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-86-14206294a062>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mcount\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[0mtime_list\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./sms.txt\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"r\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mnum_of_lines\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1000\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[0mcity\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"./city.csv\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: './sms.txt'"
     ]
    }
   ],
   "source": [
    "\"\"\"Evaluate agent's performance after Q-learning\"\"\"\n",
    "\n",
    "# 1) We need to take text drom \"sms.txt\" and fetch pickup and drop from it.\n",
    "# 2) Generate the random state from an enviroment and change the pick up and drop as the fetched one\n",
    "# 3) Evaluate you q_table performance on all the texts given in sms.txt.\n",
    "# 4) Have a check if the fetched pickup, drop is not matching with original pickup, drop using orig.csv\n",
    "# 5) If fetched pickup or/and drop does not match with the original, add penality and reward -10\n",
    "# 6) Calculate the Total reward, penalities, Wrong pickup/drop predicted and Average time steps per episode.\n",
    "\n",
    "total_epochs, total_penalties, total_reward, wrong_predictions = 0, 0, 0, 0\n",
    "\n",
    "\n",
    "count = 0\n",
    "time_list = []\n",
    "f = open(\"./sms.txt\", \"r\")\n",
    "num_of_lines = 1000\n",
    "city = pd.read_csv(\"./city.csv\")\n",
    "loc_dict = create_loc_dict(city)\n",
    "line_num = 0\n",
    "for line in f:\n",
    "    state = env.reset()\n",
    "    \n",
    "    epochs, penalties, reward = 0, 0, 0\n",
    "    \n",
    "    done = False\n",
    "    \n",
    "    fetched_origin, fetched_destination, fetched_pickup_time = fetch_pickup_drop(line)\n",
    "    #print(\"fetched_origin: \" + fetched_origin + \", fetched_destination: \" + fetched_destination)\n",
    "    isCorrect = check_pick_up_drop_correction(fetched_origin, fetched_destination, line_num)\n",
    "    \n",
    "    if(isCorrect):\n",
    "        origin_mapping = loc_dict[fetched_origin.strip()]\n",
    "        destination_mapping = loc_dict[fetched_destination.strip()]\n",
    "        state = env.encode(3, 1, origin_mapping, destination_mapping)\n",
    "        env.s = state\n",
    "        \n",
    "    \n",
    "        while not done:\n",
    "            action = np.argmax(q_table[state])\n",
    "            \n",
    "            state, reward, done, info = env.step(action)\n",
    "        \n",
    "            if reward == -10:\n",
    "                penalties += 1\n",
    "            \n",
    "            epochs += 1\n",
    "    else:\n",
    "        wrong_predictions += 1\n",
    "        reward = -10\n",
    "        penalties += 1\n",
    "        \n",
    "    total_epochs += epochs\n",
    "    total_penalties += penalties\n",
    "    \n",
    "    total_reward += reward\n",
    " ##Write your code here\n",
    "    line_num += 1\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Results after {num_of_lines} episodes:\")\n",
    "print(f\"Average timesteps per episode: {total_epochs / num_of_lines}\")\n",
    "print(f\"Average penalties per episode: {total_penalties / num_of_lines}\")\n",
    "print(f\"Total number of wrong predictions\", wrong_predictions)\n",
    "print(\"Total Reward is\", total_reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
