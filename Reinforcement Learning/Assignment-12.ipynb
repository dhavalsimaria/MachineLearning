{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The model of the maze environment is provided for your reference.\n",
    "\n",
    "ACTION_UP    =  0\n",
    "ACTION_RIGHT =  1\n",
    "ACTION_DOWN  =  2\n",
    "ACTION_LEFT  =  3\n",
    "\n",
    "class GridWorld(object):\n",
    "    def __init__(self,shape=[5,5]):\n",
    "        self.shape = shape\n",
    "\n",
    "\n",
    "        numStates  = shape[0] * shape[1]\n",
    "        numActions = 4\n",
    "        self.numStates = numStates\n",
    "        self.numActions = numActions\n",
    "        \n",
    "        xmax = shape[0]\n",
    "        ymax = shape[1]\n",
    "\n",
    "        grid = np.arange(numStates).reshape(shape)\n",
    "\n",
    "        Model = {}\n",
    "\n",
    "        x_indices = np.arange(xmax)\n",
    "        y_indices = np.arange(ymax)\n",
    "\n",
    "        for x in x_indices:\n",
    "            for y in y_indices:\n",
    "                state = y + x*(xmax)\n",
    "                #print(x,y,state)\n",
    "                Model[state] ={action:[] for action in np.arange(numActions)}\n",
    "\n",
    "                is_terminal_state = lambda state : state == 0 or state == (numStates-1)\n",
    "                reward = 0.0 if is_terminal_state(state) else -1.0\n",
    "\n",
    "\n",
    "                if is_terminal_state(state):\n",
    "                    Model[state][ACTION_UP] = [(1.0,state,reward,True)]\n",
    "                    Model[state][ACTION_RIGHT] = [(1.0,state,reward,True)]\n",
    "                    Model[state][ACTION_DOWN] = [(1.0,state,reward,True)]\n",
    "                    Model[state][ACTION_LEFT] = [(1.0,state,reward,True)]\n",
    "                else:\n",
    "                    next_state = {}\n",
    "                    next_state[ACTION_UP] = state if x == 0 else state - ymax\n",
    "                    next_state[ACTION_RIGHT] = state if y == ymax-1 else state +1\n",
    "                    next_state[ACTION_DOWN] = state if x == xmax-1 else state + ymax\n",
    "                    next_state[ACTION_LEFT] = state if y == 0 else state -1 \n",
    "                    Model[state][ACTION_UP] = [(1.0,next_state[ACTION_UP] ,reward,is_terminal_state(next_state[ACTION_UP]))]\n",
    "                    Model[state][ACTION_RIGHT] = [(1.0,next_state[ACTION_RIGHT],reward,is_terminal_state(next_state[ACTION_RIGHT]))]\n",
    "                    Model[state][ACTION_DOWN] = [(1.0,next_state[ACTION_DOWN],reward,is_terminal_state(next_state[ACTION_DOWN]))]\n",
    "                    Model[state][ACTION_LEFT] = [(1.0,next_state[ACTION_LEFT],reward,is_terminal_state(next_state[ACTION_LEFT]))]\n",
    "        self.model = Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write a function to calculate the value for all the actions in a given state  \n",
    "\n",
    "def value_iteration(env, theta=0.0001, discount_factor=1.0):\n",
    "    def compute_value_fn_update(state, vale_fn):\n",
    "        value_fn_update = np.zeros(env.numActions)\n",
    "        for action in range(env.numActions):\n",
    "            for prob, next_state, reward, done in env.model[state][action]:\n",
    "                value_fn_update[action] += prob*(reward + discount_factor * value_fn[next_state])\n",
    "        return value_fn_update\n",
    "        \n",
    "    value_fn = np.zeros(env.numStates)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for state in range(env.numStates):\n",
    "            action_values = compute_value_fn_update(state, value_fn)\n",
    "            best_action_value = np.max(action_values)\n",
    "            delta = max(delta, np.abs(best_action_value - value_fn[state]))\n",
    "            value_fn[state] - best_action_value\n",
    "        \n",
    "        if delta < theta:\n",
    "            break\n",
    "        \n",
    "    policy = np.zeros([env.numStates, env.numActions])\n",
    "    for state in rnage(env.numStates):\n",
    "        A - compute_value_fn_update(state, value_fn)\n",
    "        best_action = np.argmax(A)\n",
    "        policy[state, best_action] = 1.0\n",
    "        \n",
    "    return policy, value_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the code to create a deterministic policy by using the optimal value function. Print the optimal policy.\n",
    "env = GridWorld()\n",
    "policy, value_fn = value_iteration(env)\n",
    "print(\"Policy grid (0-Up, 1-Right, 2-Down, 3-Left): \")\n",
    "print(np.reshape(np.argmax(policy, axis=1), env.shape))\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the final output as described in the sample format given below \n",
    "\n",
    "# data= np.array([[1, 2, 3, 4, 5], [0, 0, 4, 1, 2],[3, 4, 1, 0, 2],[2, 1, 3, 4, 5],[2, 1, 3, 4, 5]])\n",
    "data = np.array(policy)\n",
    "output=pd.DataFrame(data)\n",
    "output.to_csv('/code/output/output.csv', header=False, index=False) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
